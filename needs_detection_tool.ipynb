{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9fc074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs detection tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217e4c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import os, sys, json, time, ahocorasick\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import defaultdict\n",
    "import glob, os\n",
    "from pathlib import Path\n",
    "tokenizer = TweetTokenizer()\n",
    "from tqdm import tqdm\n",
    "\n",
    "NEED_TERMS = ['needs', 'supplies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea79b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/shangjingbo1226/AutoPhrase.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d13ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(line):\n",
    "    separated = _.split(\",\", 1)\n",
    "    return (separated[0], separated[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d1ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read phrases file\n",
    "def read_phrases(PHRASES_FILE):\n",
    "    # ahocorasick: faster way of searching for phrases in text\n",
    "    phrase_search = {}\n",
    "    for i in range(MIN_PHRASE_LENGTH, MAX_PHRASE_LENGTH + 1):\n",
    "        phrase_search[i] = ahocorasick.Automaton()\n",
    "\n",
    "    all_phrases = []\n",
    "    with open(PHRASES_FILE, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split('\\t')\n",
    "            if len(line) > 1 and float(line[0]) >= PHRASE_THRESHOLD:\n",
    "                if line[1].endswith(\"'s\"):\n",
    "                    line[1] = line[1][:-2]\n",
    "                \n",
    "                tokens = tokenizer.tokenize(line[1])\n",
    "                if len(tokens) <= MAX_PHRASE_LENGTH and len(tokens) >= MIN_PHRASE_LENGTH:\n",
    "                    phrase = '-'.join(tokens)\n",
    "\n",
    "                    phrase_search[len(tokens)].add_word(line[1], (line[1], phrase))\n",
    "\n",
    "                    all_phrases.append(phrase)\n",
    "\n",
    "    for i in range(MIN_PHRASE_LENGTH, MAX_PHRASE_LENGTH + 1):\n",
    "        phrase_search[i].make_automaton()\n",
    "                    \n",
    "    return all_phrases, phrase_search\n",
    "\n",
    "# annotate phrases in text so they are kept as unigrams\n",
    "def annotate_phrases(raw_sent, phrases):\n",
    "    phrase_sent = raw_sent.lower()\n",
    "    for i in range(MAX_PHRASE_LENGTH , MIN_PHRASE_LENGTH - 1, -1):\n",
    "        if len(phrases[i]) > 0:\n",
    "            phrases_in_sent = {}\n",
    "            for end_index, (phrase, combined_phrase) in phrases[i].iter(phrase_sent):\n",
    "                phrases_in_sent[phrase] = combined_phrase\n",
    "\n",
    "            for phrase in sorted(phrases_in_sent, key = len, reverse = True):\n",
    "                phrase_sent = phrase_sent.replace(phrase, phrases_in_sent[phrase])\n",
    "\n",
    "    return phrase_sent\n",
    "\n",
    "def get_nouns(vocab_pos, phrases):\n",
    "    nouns = set()\n",
    "    for word, tags in vocab_pos.items():\n",
    "        # if word == 'needed':\n",
    "        #     print(tags)\n",
    "        # if word is more frequently used as a noun, add it to list of nouns\n",
    "        if max(tags, key = tags.get) in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "            nouns.add(word)\n",
    "\n",
    "    for phrase in phrases:\n",
    "        tokens = phrase.split('-')\n",
    "        final_token = tokens[-1]\n",
    "\n",
    "        if final_token in nouns:\n",
    "            nouns.add(phrase)\n",
    "\n",
    "    return nouns\n",
    "\n",
    "# find all nouns closest to NEED_TERMS\n",
    "def get_ranked_needs(emb_model, nouns, output_file, top = 100):\n",
    "    top_noun_count = 0\n",
    "    with open(os.path.join(output_file), 'w') as o:\n",
    "        for (term, score) in emb_model.wv.most_similar(positive = NEED_TERMS, topn = top * 2):\n",
    "            if term in nouns:\n",
    "                top_noun_count += 1\n",
    "                o.write(term + '\\n')\n",
    "\n",
    "                if top_noun_count == top:\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618494d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    start_time = time.time()\n",
    "\n",
    "    # set input file\n",
    "    input_file_path = \"Needs_Project_S23/gsDataTotal.txt\"\n",
    "    output_folder_path = \"Needs_Project_S23/needs_information\"\n",
    "\n",
    "    INPUT_FILE = input_file_path #sys.argv[1]\n",
    "    OUTPUT_FOLDER = output_folder_path #sys.argv[2]\n",
    "\n",
    "\n",
    "    OUTPUT_FILE = os.path.join(OUTPUT_FOLDER, 'total_sentences.json')\n",
    "    NEEDS_FILE = os.path.join(OUTPUT_FOLDER, 'total_priority-needs.txt')\n",
    "\n",
    "    # set phrases information\n",
    "    phrases_file_path = \"AutoPhrase/models/DBLP/AutoPhrase.txt\"\n",
    "\n",
    "    PHRASES_FILE = phrases_file_path #sys.argv[3]\n",
    "\n",
    "    PHRASE_THRESHOLD = 0.8\n",
    "    MIN_PHRASE_LENGTH = 2\n",
    "    MAX_PHRASE_LENGTH = 5\n",
    "\n",
    "    if len(sys.argv) < 5:\n",
    "        TOPN = 100\n",
    "\n",
    "    all_phrases, phrase_search = read_phrases(PHRASES_FILE)\n",
    "\n",
    "    print('Initialization took {} seconds'.format((time.time() - start_time)))\n",
    "\n",
    "    # read tweets and split into sentences and tokens\n",
    "    print('Performing phrase annotation and POS tagging...')\n",
    "\n",
    "    tagging_start_time = time.time()\n",
    "\n",
    "    all_sents = []\n",
    "    vocab_pos = {}\n",
    "    # row = 0\n",
    "    with open(INPUT_FILE, 'r') as f:\n",
    "        with open(OUTPUT_FILE, 'w') as o:\n",
    "            for _ in tqdm(f): # input file should be line by line\n",
    "                try:\n",
    "                    tweet_id, line = split(_)\n",
    "                except IndexError:\n",
    "                    continue\n",
    "\n",
    "                sentences = sent_tokenize(line)\n",
    "                for raw_sent in sentences:\n",
    "                    raw_tokens = tokenizer.tokenize(raw_sent)\n",
    "\n",
    "                    phrase_sent = annotate_phrases(raw_sent, phrase_search)\n",
    "                    phrase_tokens = tokenizer.tokenize(phrase_sent)\n",
    "\n",
    "                    pos = pos_tag(raw_tokens)\n",
    "\n",
    "                    json.dump({\n",
    "                        'raw_sent' : raw_sent,\n",
    "                        'raw_tokens' : raw_tokens,\n",
    "                        'phrase_sent' : phrase_sent,\n",
    "                        'phrase_tokens' : phrase_tokens,\n",
    "                        'pos' : pos,\n",
    "                        'tweet_id': tweet_id\n",
    "                        }, o)\n",
    "                    o.write('\\n')\n",
    "\n",
    "                    all_sents.append(phrase_tokens)\n",
    "\n",
    "                    for (word, tag) in pos:\n",
    "                        if word not in vocab_pos:\n",
    "                            vocab_pos[word.lower()] = defaultdict(int)\n",
    "                        vocab_pos[word.lower()][tag] += 1 \n",
    "\n",
    "                # row = row + 1\n",
    "                # if row == 2000: break\n",
    "\n",
    "    nouns = get_nouns(vocab_pos, all_phrases)\n",
    "\n",
    "    print('Tagging took {} seconds'.format((time.time() - tagging_start_time)))\n",
    "\n",
    "    # generate word embeddings\n",
    "    print('Generating word embeddings...')\n",
    "    embedding_start_time = time.time()\n",
    "\n",
    "    model = Word2Vec(sentences = all_sents)\n",
    "    model.save(os.path.join(OUTPUT_FOLDER, 'total_word2vec.model'))\n",
    "    print('Embedding took {} seconds'.format((time.time() - embedding_start_time)))\n",
    "\n",
    "    print('Identifying needs and priorities...')\n",
    "    detection_start_time = time.time()\n",
    "\n",
    "    get_ranked_needs(model, nouns, NEEDS_FILE, TOPN)\n",
    "\n",
    "    print('Needs detection took {} seconds'.format((time.time() - detection_start_time)))\n",
    "\n",
    "    print('END: whole process took {} seconds'.format((time.time() - start_time)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
