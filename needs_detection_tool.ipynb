{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9fc074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs detection tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "217e4c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import os, sys, json, time, ahocorasick\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "NEED_TERMS = ['needs', 'supplies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89d1ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read phrases file\n",
    "def read_phrases(PHRASES_FILE):\n",
    "    # ahocorasick: faster way of searching for phrases in text\n",
    "    phrase_search = {}\n",
    "    for i in range(MIN_PHRASE_LENGTH, MAX_PHRASE_LENGTH + 1):\n",
    "        phrase_search[i] = ahocorasick.Automaton()\n",
    "\n",
    "    all_phrases = []\n",
    "    with open(PHRASES_FILE, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split('\\t')\n",
    "            if len(line) > 1 and float(line[0]) >= PHRASE_THRESHOLD:\n",
    "                if line[1].endswith(\"'s\"):\n",
    "                    line[1] = line[1][:-2]\n",
    "                \n",
    "                tokens = tokenizer.tokenize(line[1])\n",
    "                if len(tokens) <= MAX_PHRASE_LENGTH and len(tokens) >= MIN_PHRASE_LENGTH:\n",
    "                    phrase = '-'.join(tokens)\n",
    "\n",
    "                    phrase_search[len(tokens)].add_word(line[1], (line[1], phrase))\n",
    "\n",
    "                    all_phrases.append(phrase)\n",
    "\n",
    "    for i in range(MIN_PHRASE_LENGTH, MAX_PHRASE_LENGTH + 1):\n",
    "        phrase_search[i].make_automaton()\n",
    "                    \n",
    "    return all_phrases, phrase_search\n",
    "\n",
    "# annotate phrases in text so they are kept as unigrams\n",
    "def annotate_phrases(raw_sent, phrases):\n",
    "    phrase_sent = raw_sent.lower()\n",
    "    for i in range(MAX_PHRASE_LENGTH , MIN_PHRASE_LENGTH - 1, -1):\n",
    "        if len(phrases[i]) > 0:\n",
    "            phrases_in_sent = {}\n",
    "            for end_index, (phrase, combined_phrase) in phrases[i].iter(phrase_sent):\n",
    "                phrases_in_sent[phrase] = combined_phrase\n",
    "\n",
    "            for phrase in sorted(phrases_in_sent, key = len, reverse = True):\n",
    "                phrase_sent = phrase_sent.replace(phrase, phrases_in_sent[phrase])\n",
    "\n",
    "    return phrase_sent\n",
    "\n",
    "def get_nouns(vocab_pos, phrases):\n",
    "    nouns = set()\n",
    "    for word, tags in vocab_pos.items():\n",
    "        # if word == 'needed':\n",
    "        #     print(tags)\n",
    "        # if word is more frequently used as a noun, add it to list of nouns\n",
    "        if max(tags, key = tags.get) in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "            nouns.add(word)\n",
    "\n",
    "    for phrase in phrases:\n",
    "        tokens = phrase.split('-')\n",
    "        final_token = tokens[-1]\n",
    "\n",
    "        if final_token in nouns:\n",
    "            nouns.add(phrase)\n",
    "\n",
    "    return nouns\n",
    "\n",
    "# find all nouns closest to NEED_TERMS\n",
    "def get_ranked_needs(emb_model, nouns, output_file, top = 100):\n",
    "    top_noun_count = 0\n",
    "    with open(os.path.join(output_file), 'w') as o:\n",
    "        for (term, score) in emb_model.wv.most_similar(positive = NEED_TERMS, topn = top * 2):\n",
    "            if term in nouns:\n",
    "                top_noun_count += 1\n",
    "                o.write(term + '\\n')\n",
    "\n",
    "                if top_noun_count == top:\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d7cfc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoPhrase.txt             segmentation.model\r\n",
      "AutoPhrase_multi-words.txt segmentation.txt\r\n",
      "AutoPhrase_single-word.txt token_mapping.txt\r\n",
      "language.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls AutoPhrase/models/DBLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b46c16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization took 0.9043059349060059 seconds\n",
      "Performing phrase annotation and POS tagging...\n",
      "Tagging took 24.65677523612976 seconds\n",
      "Generating word embeddings...\n",
      "Embedding took 1.1931097507476807 seconds\n",
      "Identifying needs and priorities...\n",
      "Needs detection took 0.007052183151245117 seconds\n",
      "END: whole process took 26.76171898841858 seconds\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    start_time = time.time()\n",
    "\n",
    "    # set input file\n",
    "    input_file_path = \"sentences.txt\"\n",
    "    output_folder_path = \"needs_information\"\n",
    "    \n",
    "    INPUT_FILE = input_file_path #sys.argv[1]\n",
    "    OUTPUT_FOLDER = output_folder_path #sys.argv[2]\n",
    "\n",
    "    \n",
    "    OUTPUT_FILE = os.path.join(OUTPUT_FOLDER, 'sentences.json')\n",
    "    NEEDS_FILE = os.path.join(OUTPUT_FOLDER, 'priority-needs.txt')\n",
    "\n",
    "    # set phrases information\n",
    "    phrases_file_path = \"AutoPhrase/models/DBLP/AutoPhrase.txt\"\n",
    "    \n",
    "    PHRASES_FILE = phrases_file_path #sys.argv[3]\n",
    "    \n",
    "    PHRASE_THRESHOLD = 0.8\n",
    "    MIN_PHRASE_LENGTH = 2\n",
    "    MAX_PHRASE_LENGTH = 5\n",
    "\n",
    "    if len(sys.argv) < 5:\n",
    "        TOPN = 100\n",
    "\n",
    "    tokenizer = TweetTokenizer()\n",
    "\n",
    "    all_phrases, phrase_search = read_phrases(PHRASES_FILE)\n",
    "\n",
    "    print('Initialization took {} seconds'.format((time.time() - start_time)))\n",
    "\n",
    "    # read tweets and split into sentences and tokens\n",
    "    print('Performing phrase annotation and POS tagging...')\n",
    "    \n",
    "    tagging_start_time = time.time()\n",
    "\n",
    "    all_sents = []\n",
    "    vocab_pos = {}\n",
    "    # row = 0\n",
    "    with open(INPUT_FILE, 'r') as f:\n",
    "        with open(OUTPUT_FILE, 'w') as o:\n",
    "            for _ in f: # input file should be line by line\n",
    "                try:\n",
    "                    tweet_id, line = _.split(\",\", 1)\n",
    "                except IndexError:\n",
    "                    tweet_id = '0'\n",
    "                    line = _\n",
    "                sentences = sent_tokenize(line)\n",
    "                for raw_sent in sentences:\n",
    "                    raw_tokens = tokenizer.tokenize(raw_sent)\n",
    "\n",
    "                    phrase_sent = annotate_phrases(raw_sent, phrase_search)\n",
    "                    phrase_tokens = tokenizer.tokenize(phrase_sent)\n",
    "\n",
    "                    pos = pos_tag(raw_tokens)\n",
    "                    \n",
    "                    json.dump({\n",
    "                        'raw_sent' : raw_sent,\n",
    "                        'raw_tokens' : raw_tokens,\n",
    "                        'phrase_sent' : phrase_sent,\n",
    "                        'phrase_tokens' : phrase_tokens,\n",
    "                        'pos' : pos,\n",
    "                        'tweet_id': tweet_id\n",
    "                        }, o)\n",
    "                    o.write('\\n')\n",
    "\n",
    "                    all_sents.append(phrase_tokens)\n",
    "\n",
    "                    for (word, tag) in pos:\n",
    "                        if word not in vocab_pos:\n",
    "                            vocab_pos[word.lower()] = defaultdict(int)\n",
    "                        vocab_pos[word.lower()][tag] += 1 \n",
    "\n",
    "                # row = row + 1\n",
    "                # if row == 2000: break\n",
    "\n",
    "    nouns = get_nouns(vocab_pos, all_phrases)\n",
    "\n",
    "    print('Tagging took {} seconds'.format((time.time() - tagging_start_time)))\n",
    "\n",
    "    # generate word embeddings\n",
    "    print('Generating word embeddings...')\n",
    "    embedding_start_time = time.time()\n",
    "\n",
    "    model = Word2Vec(sentences = all_sents)\n",
    "    model.save(os.path.join(OUTPUT_FOLDER, 'word2vec.model'))\n",
    "    print('Embedding took {} seconds'.format((time.time() - embedding_start_time)))\n",
    "\n",
    "    print('Identifying needs and priorities...')\n",
    "    detection_start_time = time.time()\n",
    "\n",
    "    get_ranked_needs(model, nouns, NEEDS_FILE, TOPN)\n",
    "    \n",
    "    print('Needs detection took {} seconds'.format((time.time() - detection_start_time)))\n",
    "\n",
    "    print('END: whole process took {} seconds'.format((time.time() - start_time)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
