{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5ff70f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, json\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "import spacy, pyinflect, nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from itertools import chain\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5906b05",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/alexstratton2121/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/alexstratton2121/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download(\"wordnet\")\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0303daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(line):\n",
    "    separated = line.split(\",\", 1)\n",
    "    return (separated[0], separated[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15753489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_words(word):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    similar, lemmas = [], []\n",
    "    for word_set in wn.synsets(\"need\", pos=wn.VERB):\n",
    "        for related in word_set.lemma_names():\n",
    "            similar.append(related)\n",
    "        \n",
    "        for lemma in word_set.lemmas():\n",
    "            lemmas.append(lemma)\n",
    "        \n",
    "    verbs = ' '.join(set(similar))\n",
    "    spacy_doc = nlp(verbs)\n",
    "    verb_types = [\"VB\", \"VBG\", \"VBD\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "    verbs = [(token._.inflect(verb_type), verb_type) for token in spacy_doc \n",
    "                  for verb_type in verb_types if '_' not in str(token)]\n",
    "    \n",
    "    nouns = []\n",
    "    for lemma in lemmas:\n",
    "        for related_form in lemma.derivationally_related_forms():\n",
    "            nouns.append(related_form.name().split('.', 1)[0])\n",
    "    nouns = ' '.join(set(nouns))\n",
    "    spacy_doc = nlp(nouns)\n",
    "    noun_types = [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]\n",
    "    nouns = [(token._.inflect(noun_type), noun_type) for token in spacy_doc \n",
    "                  for noun_type in noun_types]\n",
    "    nouns = [noun for noun in nouns if noun[0]]\n",
    "    \n",
    "    return verbs + nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d77a1e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_detection_0(sentence, needs_list, most_similar):\n",
    "    pos_tagged = pos_tag(sentence.split())\n",
    "    if len([word for word in pos_tagged if word in most_similar]) != 0:\n",
    "        counter = Counter([need[0] for need in pos_tagged\n",
    "                           if (need[0] in needs_list and need[1] in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"])])\n",
    "        return counter\n",
    "    \n",
    "    return Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f958e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_detection_1(sentence, needs_list):\n",
    "    pos_tagged = pos_tag(sentence.split())\n",
    "    counter = Counter([need[0] for need in pos_tagged\n",
    "                           if (need[0] in needs_list and need[1] in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"])])\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a7e1f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Priority needs list for expanded data set\n",
    "priority_needs = []\n",
    "with open(\"priority_needs_all.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        priority_needs.append(line.strip())\n",
    "        \n",
    "priority_needs[33] = priority_needs[33][:3] + 'i' + priority_needs[33][3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "65d8c7e6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [62]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      6\u001b[0m     tweets_for_day \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f]\n\u001b[0;32m----> 7\u001b[0m     needs_for_day \u001b[38;5;241m=\u001b[39m [frequency_detection_0(sent, priority_needs, sim_words) \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m tweets_for_day]\n\u001b[1;32m      8\u001b[0m     needs_for_day \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m needs_for_day \u001b[38;5;28;01mif\u001b[39;00m x], Counter())\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mstr\u001b[39m(Path\u001b[38;5;241m.\u001b[39mcwd()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/needs_frequency_information/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m file[\u001b[38;5;241m51\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_frequency.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m o:\n",
      "Input \u001b[0;32mIn [62]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      6\u001b[0m     tweets_for_day \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f]\n\u001b[0;32m----> 7\u001b[0m     needs_for_day \u001b[38;5;241m=\u001b[39m [\u001b[43mfrequency_detection_0\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpriority_needs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msim_words\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m tweets_for_day]\n\u001b[1;32m      8\u001b[0m     needs_for_day \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m needs_for_day \u001b[38;5;28;01mif\u001b[39;00m x], Counter())\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mstr\u001b[39m(Path\u001b[38;5;241m.\u001b[39mcwd()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/needs_frequency_information/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m file[\u001b[38;5;241m51\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_frequency.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m o:\n",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36mfrequency_detection_0\u001b[0;34m(sentence, needs_list, most_similar)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrequency_detection_0\u001b[39m(sentence, needs_list, most_similar):\n\u001b[0;32m----> 2\u001b[0m     pos_tagged \u001b[38;5;241m=\u001b[39m \u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m([word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m pos_tagged \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m most_similar]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      4\u001b[0m         counter \u001b[38;5;241m=\u001b[39m Counter([need[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m need \u001b[38;5;129;01min\u001b[39;00m pos_tagged\n\u001b[1;32m      5\u001b[0m                            \u001b[38;5;28;01mif\u001b[39;00m (need[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m needs_list \u001b[38;5;129;01mand\u001b[39;00m need[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNNS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNNP\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNNPS\u001b[39m\u001b[38;5;124m\"\u001b[39m])])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/tag/__init__.py:166\u001b[0m, in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03mUse NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03mtag the given list of tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m:rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    165\u001b[0m tagger \u001b[38;5;241m=\u001b[39m _get_tagger(lang)\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/tag/__init__.py:123\u001b[0m, in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens: expected a list of strings, got a string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 123\u001b[0m     tagged_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtagger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tagset:  \u001b[38;5;66;03m# Maps to the specified tagset.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m lang \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/tag/perceptron.py:187\u001b[0m, in \u001b[0;36mPerceptronTagger.tag\u001b[0;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tag:\n\u001b[1;32m    186\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_features(i, word, context, prev, prev2)\n\u001b[0;32m--> 187\u001b[0m     tag, conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_conf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m output\u001b[38;5;241m.\u001b[39mappend((word, tag, conf) \u001b[38;5;28;01mif\u001b[39;00m return_conf \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (word, tag))\n\u001b[1;32m    190\u001b[0m prev2 \u001b[38;5;241m=\u001b[39m prev\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/tag/perceptron.py:66\u001b[0m, in \u001b[0;36mAveragedPerceptron.predict\u001b[0;34m(self, features, return_conf)\u001b[0m\n\u001b[1;32m     64\u001b[0m     weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[feat]\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m label, weight \u001b[38;5;129;01min\u001b[39;00m weights\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 66\u001b[0m         scores[label] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m value \u001b[38;5;241m*\u001b[39m weight\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Do a secondary alphabetic sort, for stability\u001b[39;00m\n\u001b[1;32m     69\u001b[0m best_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m label: (scores[label], label))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_files = sorted(glob.glob(os.path.join(str(Path.cwd()) + '/gsData', \"*.txt\")))\n",
    "sim_words = similar_words(\"need\")\n",
    "for file in all_files:\n",
    "    needs_for_day = []\n",
    "    with open(file) as f:\n",
    "        tweets_for_day = [line.replace('\\n', '') for line in f]\n",
    "        needs_for_day = [frequency_detection_0(sent, priority_needs, sim_words) for sent in tweets_for_day]\n",
    "        needs_for_day = sum([x for x in needs_for_day if x], Counter())\n",
    "        with open(str(Path.cwd()) + \"/needs_frequency_information/\" + file[51:-4] + \"_frequency.json\", 'w') as o:\n",
    "            json.dump(dict(needs_for_day), o)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "needs_venv",
   "language": "python",
   "name": "needs_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
